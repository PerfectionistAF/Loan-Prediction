https://data-flair.training/blogs/python-ml-data-preprocessing/
https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/
https://rapidapi.com/search/data-mining
https://app.gitbook.com/o/4z13MuE7Im6Edv8q5lEA/s/SluzzTrBchuBG1iJk66V/data-mining-project
https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html
https://www.malicksarr.com/remove-missing-values-in-python/#:~:text=The%20simplest%20and%20fastest%20way,frame%20containing%20an%20empty%20value.


1- cleaning

2- remove outliers (use z-score normalization to decrease the
impact of outliers, then decide whether they are worht keeping or deleting)
source: https://www.geeksforgeeks.org/data-normalization-in-data-mining/
(knn or isolation forest for anomaly detection)

3- discretization
4- normalization (research)
5- feature selection (pca, etc, curse of dimensionality)

- note: finish visualization
we might want to reduce skewness to assist in plotting. this can be done using data
scaling/ normalization

"
The overall goal of transforming our data is to create a more normal (*Gaussian*) 
distribution aka a bell curve. In general, normal distributions tend to produce better 
results in a model because there are about equal observations above and below the mean 
and the mean and median are the same. Models run under the assumption your data is 
normally distributed.
"
source: https://medium.com/@isalindgren313/transformations-scaling-and-normalization-420b2be12300








external datasets: 
https://www.openintro.org/data/index.php?data=loans_full_schema
https://data.gov.ie/dataset/overall-loan-approvals-by-year/resource/23c06cd8-8d58-4b2f-82b3-95f014f35b57